package mapreduce.page;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class JobSubmitter {
	
	public static void main(String[] args) throws Exception{
		// bWindows Local狠B妗ANぃノ]m把计
		// 1. iHqL[更classpathU*-site.xmlゅン秆R把计
		// ぃ[conf.addResource()Ahdfsu|郯圣[更core-site.xmlMhdfs-site.xmlAㄤLぃ{o
		// 癸蟥浈L*-site.xmlA惠nzLcon.addResource()~啷拷T[更
		Configuration conf = new Configuration();
		conf.addResource("*-site.xml");
		
		// 2. qLNX]m把计
//		conf.setInt("top.n", 3); // ]w郗q把计WM
		
		// 3. qL妮┦tmゅン莉把计(tmtopn.propertiesゅン)Ai把σe豹酣窑l
		
		// confQ矢栓Job柑Aぇ岽NiHqJobo勖霭鸭(ㄒp:qcontexto)
		Job job = Job.getInstance(conf);
		
		job.setJarByClass(JobSubmitter.class);
		
		job.setMapperClass(PageTopnMapper.class);
		job.setReducerClass(PageTopnReducer.class);
		
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IntWritable.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		
		FileInputFormat.setInputPaths(job, new Path("D:/Apache Ecosystem/flow/input"));
		FileOutputFormat.setOutputPath(job, new Path("D:/Apache Ecosystem/flow/output"));
		
		job.waitForCompletion(true);
	}

}
